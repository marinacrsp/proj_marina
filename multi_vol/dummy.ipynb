{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fastmri\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from data_utils import *\n",
    "from datasets import *\n",
    "from fastmri.data.transforms import tensor_to_complex_np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model import *\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14534400\n"
     ]
    }
   ],
   "source": [
    "files = '/itet-stor/mcrespo/bmicdatasets-originals/Originals/fastMRI/brain/multicoil_train/'\n",
    "\n",
    "dataset = KCoordDataset(files, n_volumes=3, n_slices=3, with_mask=False)\n",
    "print(len(dataset))\n",
    "# loader_config = config[\"dataloader\"]\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, collate_fn=collate_fn, pin_memory=PIN_MEMORY, worker_init_fn=seed_worker, generator=RS_TORCH)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=120_000,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "n_slices = 3\n",
    "\n",
    "\n",
    "for vol_id in dataloader.dataset.metadata.keys():\n",
    "    file = dataloader.dataset.metadata[vol_id][\"file\"]\n",
    "    with h5py.File(file, \"r\") as hf:\n",
    "        ground_truth.append(\n",
    "            hf[\"reconstruction_rss\"][()][: n_slices]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23050/2161962825.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_checkpoint, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = '/scratch_net/ken/mcrespo/proj_marina/logs/multivol/2024-11-11_09h28m59s/checkpoints/epoch_0999.pt'  # TODO: SET (OR LEAVE COMMENTED).\n",
    "\n",
    "\n",
    "gamma = 0.1\n",
    "sigma = 0.01\n",
    "lr = 5.e-6\n",
    "embedding_dim = 512\n",
    "\n",
    "OPTIMIZER_CLASSES = {\n",
    "    \"Adam\": Adam,\n",
    "    \"AdamW\": AdamW,\n",
    "    \"SGD\": SGD,\n",
    "}\n",
    "\n",
    "LOSS_CLASSES = {\n",
    "    \"MAE\": MAELoss,\n",
    "    \"DMAE\": DMAELoss,\n",
    "    \"MSE\": MSELoss,\n",
    "    \"MSEDist\": MSEDistLoss,\n",
    "    \"HDR\": HDRLoss,\n",
    "    \"LogL2\": LogL2Loss,\n",
    "    \"MSEL2\": MSEL2Loss,\n",
    "}\n",
    "\n",
    "\n",
    "model = Siren(hidden_dim=512, embedding_dim=512, L =10, n_layers=8, out_dim=2)\n",
    "# Load checkpoint.\n",
    "model_state_dict = torch.load(model_checkpoint, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_state_dict[\"model_state_dict\"])\n",
    "print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "# Only embeddings are optimized.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "embeddings = torch.nn.Embedding(\n",
    "    len(dataset.metadata), embedding_dim\n",
    ")\n",
    "torch.nn.init.normal_(\n",
    "    embeddings.weight.data, 0.0, sigma\n",
    ")\n",
    "optimizer = OPTIMIZER_CLASSES[\"Adam\"](\n",
    "    embeddings.parameters(), lr\n",
    ")\n",
    "\n",
    "loss_fn = LOSS_CLASSES[\"MSEL2\"](gamma, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TRAINING FROM CHECKPOINT\n",
    "counter = 0\n",
    "for inputs, targets in dataloader:\n",
    "    counter += 1\n",
    "    # Inputs has dimension Nm x 5, position 0 corresponds to volID\n",
    "    coords, latent_embeddings = inputs[:, 1:], embeddings(\n",
    "        inputs[:, 0].long()\n",
    "    )\n",
    "    \n",
    "    if counter > 0:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5269e-02,  7.6233e-03,  2.2291e-02,  1.2017e-02, -1.5466e-02,\n",
      "        -6.5045e-03, -7.5966e-03, -2.1088e-02,  9.0111e-04, -1.0980e-04,\n",
      "         1.6407e-02,  1.2758e-02, -2.8559e-03,  1.3648e-02,  6.4741e-03,\n",
      "         1.7174e-02,  1.1482e-02,  6.7015e-03, -1.2104e-02, -1.2050e-02,\n",
      "         5.2339e-03, -1.9295e-02, -1.5418e-02,  1.9169e-04, -1.3232e-02,\n",
      "         9.2514e-03, -1.4164e-02, -9.4091e-03,  6.8163e-03, -3.8972e-02,\n",
      "         1.7190e-02, -2.1136e-02,  8.4777e-03, -1.2556e-02,  6.4036e-03,\n",
      "         7.4610e-03,  9.5836e-03, -9.9615e-03,  9.4567e-03,  2.5976e-02,\n",
      "         2.5606e-02, -5.6751e-03,  2.3165e-02, -1.9443e-03, -1.0315e-03,\n",
      "        -3.2236e-02, -2.5791e-03, -5.0094e-03, -5.8139e-03, -6.6907e-03,\n",
      "        -1.2893e-02, -4.3120e-03,  6.8851e-03,  1.3315e-02, -3.9535e-02,\n",
      "         1.3543e-02, -2.8399e-03,  1.1228e-02,  3.7920e-03, -2.1574e-03,\n",
      "        -1.0655e-02,  6.1027e-03, -1.4650e-02, -4.6833e-03,  1.5268e-02,\n",
      "        -1.0195e-02,  4.1723e-03,  3.5206e-02, -1.1250e-02, -5.2748e-03,\n",
      "        -1.6962e-02,  6.7900e-03, -2.0132e-03, -3.4194e-03,  2.3689e-03,\n",
      "        -8.1621e-03,  7.1437e-03,  1.0182e-02,  7.3168e-03, -1.5200e-02,\n",
      "        -4.2482e-03, -7.0845e-03,  3.2524e-03,  1.3172e-02, -2.0010e-03,\n",
      "        -1.2541e-02, -2.4842e-02,  8.0797e-03,  2.4625e-02, -2.4658e-02,\n",
      "        -2.2958e-03, -7.7492e-03,  1.5400e-02, -9.6982e-03, -6.9571e-03,\n",
      "         2.9490e-03,  7.8528e-03,  1.1590e-02, -6.3279e-03, -5.0389e-03,\n",
      "         3.5056e-03,  1.0914e-02, -3.8094e-03,  8.6156e-03,  1.1997e-02,\n",
      "         1.3928e-02,  5.0599e-03,  7.4565e-03,  9.3839e-03,  9.5082e-03,\n",
      "         6.0177e-03,  2.1143e-02,  5.7044e-03, -2.4695e-02, -1.2267e-02,\n",
      "        -2.6652e-03, -2.1062e-03, -7.7404e-03, -1.2073e-02, -1.4780e-02,\n",
      "        -1.5643e-02, -9.8499e-03, -5.6581e-03,  3.3049e-03,  1.5421e-02,\n",
      "        -1.5820e-02, -5.5444e-03,  1.1803e-02,  9.2185e-03,  1.2407e-02,\n",
      "         7.2670e-03,  1.8030e-02,  1.5427e-02,  1.3729e-02,  1.4437e-02,\n",
      "        -1.2669e-02,  5.7463e-03, -9.5125e-03, -1.4205e-02, -2.9318e-03,\n",
      "         2.3066e-02, -1.1865e-02,  2.8596e-02, -5.2638e-03,  1.5362e-02,\n",
      "        -2.4953e-02,  2.1111e-02, -1.7991e-02, -3.4788e-02, -2.2122e-02,\n",
      "        -1.0335e-02, -9.1010e-03,  1.3089e-02,  1.2781e-02, -1.4739e-02,\n",
      "        -5.5696e-03,  3.3943e-03,  1.5382e-02,  7.5915e-03,  2.4322e-02,\n",
      "         5.8004e-03, -2.2494e-03,  5.4230e-03, -3.9793e-03,  4.5588e-03,\n",
      "         1.0198e-03,  3.3429e-02, -1.0740e-02, -1.3836e-02,  5.1411e-03,\n",
      "        -6.7512e-03,  1.6124e-02,  1.0684e-02,  4.9803e-03,  2.0724e-03,\n",
      "         5.1163e-03, -4.7752e-03, -1.0420e-02,  8.5418e-03,  3.7272e-02,\n",
      "         9.5715e-03, -4.8227e-03,  1.9500e-02,  6.8883e-03, -3.7121e-02,\n",
      "         1.3619e-02, -1.4389e-02, -1.9096e-02,  1.0314e-02,  2.1498e-03,\n",
      "         7.6491e-03,  5.3045e-03, -1.0325e-02,  7.0177e-04,  1.2938e-02,\n",
      "         5.6488e-03,  1.0810e-02,  4.2246e-03,  2.5032e-02,  5.7398e-03,\n",
      "        -5.7147e-04, -2.3755e-02,  3.3649e-03,  4.3449e-03, -1.2261e-03,\n",
      "         2.0451e-02, -2.8549e-03,  1.7694e-02,  1.4437e-02,  2.2712e-02,\n",
      "         6.7824e-03,  2.6025e-02, -1.8980e-02, -2.2110e-02,  4.1764e-03,\n",
      "         1.1021e-02,  2.3343e-03,  7.8823e-03,  8.9288e-03, -1.0606e-02,\n",
      "         4.2883e-03,  1.2766e-02, -1.1409e-02,  1.1484e-03,  1.8632e-02,\n",
      "        -3.3949e-02,  1.1337e-02, -2.8101e-03,  1.9675e-02,  9.8341e-04,\n",
      "         1.1960e-02, -2.7150e-03,  1.8251e-02,  3.1365e-03, -1.4076e-02,\n",
      "        -4.7974e-03, -1.1079e-02,  2.7532e-02,  2.3324e-02,  6.1313e-03,\n",
      "         7.9746e-03,  1.1343e-02,  1.3609e-03, -1.1315e-02,  9.8658e-03,\n",
      "         1.9924e-02, -6.4288e-03,  7.4369e-03, -3.6454e-03,  2.9013e-03,\n",
      "         7.9742e-03,  1.4246e-03,  2.3723e-02, -5.9076e-03,  6.1753e-03,\n",
      "         1.9019e-02, -2.9123e-02, -9.7459e-04, -5.5062e-03, -1.0681e-02,\n",
      "         9.8306e-03, -3.7747e-03,  2.6176e-03,  1.0621e-02, -2.2874e-02,\n",
      "         2.6105e-02, -1.3349e-02, -3.9302e-03, -7.7174e-03,  8.9271e-03,\n",
      "        -8.5076e-03,  7.9083e-03,  5.4585e-03,  1.3345e-02,  2.3451e-02,\n",
      "         5.0042e-04,  7.3473e-03, -2.0381e-03,  8.9491e-03,  6.9542e-03,\n",
      "        -2.4909e-03,  1.3892e-02, -2.3722e-02,  3.6570e-02,  2.2621e-02,\n",
      "         1.0496e-02,  1.8637e-02,  9.1732e-03,  2.8358e-05, -1.0926e-02,\n",
      "         8.4083e-03, -1.3954e-02, -6.9584e-03, -8.8367e-03,  1.4817e-02,\n",
      "         1.4430e-02,  2.0417e-03, -7.6208e-03,  9.9942e-03, -6.6278e-03,\n",
      "        -1.5095e-02, -6.9032e-03, -2.2037e-02,  8.9830e-03,  1.9764e-03,\n",
      "        -6.7656e-03, -3.9463e-03,  5.1590e-03,  1.1408e-02, -1.5227e-02,\n",
      "         1.2371e-02,  1.1082e-02,  1.5077e-02,  7.4717e-03,  6.5472e-04,\n",
      "         1.4623e-03, -1.0642e-02, -2.0635e-02, -1.6473e-02, -3.2217e-03,\n",
      "         6.2022e-03,  1.6794e-02,  9.3621e-03, -2.0610e-02,  1.0007e-02,\n",
      "         2.4499e-03,  2.1387e-03, -4.6846e-03, -8.2839e-03, -1.1454e-03,\n",
      "        -2.8174e-03,  2.7700e-02,  3.9993e-03, -1.0445e-02, -1.8975e-02,\n",
      "         9.2089e-03, -9.8357e-03, -4.0082e-03,  2.0593e-03, -1.4872e-03,\n",
      "         7.5220e-03, -3.1623e-03, -7.8600e-03,  3.4736e-02,  1.0440e-02,\n",
      "        -1.1966e-02, -1.0893e-02,  1.9014e-03,  2.4164e-02, -4.7032e-03,\n",
      "         1.5175e-02,  1.0456e-02,  7.2191e-03,  2.2283e-02, -2.1461e-03,\n",
      "         1.9175e-02,  1.6976e-02, -2.1506e-02,  8.7436e-03,  1.2825e-02,\n",
      "        -1.6665e-03, -2.0240e-02, -9.3367e-04,  5.6688e-03, -2.0024e-02,\n",
      "        -5.9255e-03, -2.5155e-02,  5.5621e-04, -9.0608e-03, -1.7653e-02,\n",
      "         1.5634e-03, -1.6754e-02, -7.4772e-03,  4.0599e-03,  2.7166e-02,\n",
      "         1.9021e-02, -2.2634e-02,  2.1330e-02,  1.7734e-02,  6.9940e-03,\n",
      "        -5.4564e-04, -2.2610e-02,  1.3520e-02,  9.4239e-03,  5.9784e-04,\n",
      "        -3.2202e-02,  2.0313e-02, -1.2368e-02, -7.4135e-03,  2.4507e-03,\n",
      "         1.6601e-02, -4.7996e-03, -6.9937e-03,  2.5301e-03, -6.8695e-03,\n",
      "        -2.1691e-02, -3.7270e-05,  2.3285e-02, -1.5414e-02,  3.7102e-03,\n",
      "         1.0317e-02, -1.7492e-02,  2.9407e-04,  6.3630e-03,  2.8018e-02,\n",
      "        -6.8642e-03,  1.3628e-02,  7.0014e-03, -1.3950e-02,  6.3719e-03,\n",
      "         3.0846e-03,  5.7648e-03, -7.1331e-04,  7.6437e-03, -9.5697e-03,\n",
      "         7.9803e-03,  1.6651e-02,  1.2782e-02, -1.0547e-02,  3.5321e-03,\n",
      "         1.0054e-02, -1.3432e-03, -1.7644e-02,  8.2169e-03, -1.9779e-02,\n",
      "         4.4139e-02,  4.8227e-03,  4.9056e-03, -2.2519e-03, -2.9918e-02,\n",
      "         1.2708e-02,  7.4036e-03, -2.3170e-02, -9.3031e-05, -1.6648e-03,\n",
      "        -3.2451e-03, -9.8465e-03,  2.1784e-02, -1.2532e-02,  2.9371e-03,\n",
      "        -1.6084e-03, -1.3483e-02, -9.3637e-03,  5.9431e-03, -8.7629e-03,\n",
      "        -1.9556e-03, -7.6880e-03, -2.6552e-03, -2.6713e-02, -1.6084e-02,\n",
      "        -1.5609e-02, -6.8200e-03,  3.8541e-02, -4.4774e-03,  3.1839e-03,\n",
      "         3.3462e-02,  1.7937e-02,  5.7164e-03, -6.8961e-03,  1.0404e-02,\n",
      "         3.3898e-03,  1.9567e-02, -8.1509e-03,  2.9070e-03,  9.6063e-03,\n",
      "         3.7023e-02, -6.6245e-03,  7.7842e-04, -1.6155e-02,  2.7414e-03,\n",
      "        -1.7006e-02,  1.6808e-02,  1.4925e-02, -1.5425e-02,  1.0022e-02,\n",
      "         2.2921e-02, -2.0807e-03, -4.0245e-03, -7.4924e-03,  1.2224e-02,\n",
      "         1.7239e-02,  3.8029e-02, -2.2917e-02,  7.6523e-03, -1.4135e-02,\n",
      "         7.8389e-03,  1.0912e-02, -1.6155e-02, -2.5594e-02,  2.5381e-03,\n",
      "        -4.3883e-03,  2.3554e-02,  1.1397e-02,  2.8689e-03, -1.6685e-02,\n",
      "        -1.9141e-02, -1.2316e-02,  9.4287e-03, -3.6057e-02,  6.9343e-03,\n",
      "        -7.5733e-03, -1.6117e-02,  2.4976e-02, -4.4858e-03,  1.6714e-04,\n",
      "         6.4811e-03, -5.0547e-03,  1.4491e-02, -3.2745e-03,  2.1514e-02,\n",
      "        -1.1356e-02,  4.8049e-03], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(latent_embeddings[0] - latent_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

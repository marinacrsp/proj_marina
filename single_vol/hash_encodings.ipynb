{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 16\n",
    "n_inputs_dim = 2\n",
    "log2_hashmap_size = 17 # NOTE : This is the size of the hashtable (T)\n",
    "base_resolution = 16\n",
    "max_resolution = 320\n",
    "n_features_per_level = 2\n",
    "\n",
    "\n",
    "b = np.exp((np.log(max_resolution) - np.log(base_resolution))/(base_resolution-1))\n",
    "\n",
    "\n",
    "def _get_number_of_embeddings(self, level_idx: int) -> int:\n",
    "    \"\"\"\n",
    "    level_idx: level index\n",
    "\n",
    "    returns: number of embeddings for given level. Max number is 2**self.log2_hashmap_size\n",
    "    \"\"\"\n",
    "\n",
    "    max_size = 2**self.log2_hashmap_size\n",
    "\n",
    "    resolution = int(self.base_resolution * self.b**level_idx)\n",
    "    n_level_size = (\n",
    "        resolution + 2\n",
    "    ) ** 3  # see explanation below at 'def _to_1D(...)' why we do + 2\n",
    "\n",
    "    return min(max_size, n_level_size)\n",
    "\n",
    "embeddings = nn.ModuleList(\n",
    "    [\n",
    "        nn.Embedding(\n",
    "            _get_number_of_embeddings(i), n_features_per_level\n",
    "        )\n",
    "        for i in range(n_levels)\n",
    "    ]\n",
    ")\n",
    "\n",
    "box_offsets = torch.tensor([[i, j] for i in [0, 1] for j in [0, 1]])\n",
    "\n",
    "def _hash(coords: torch.Tensor, log2_hashmap_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: this function can process upto 7 dim coordinates\n",
    "    log2T:  logarithm of T w.r.t 2\n",
    "    \"\"\"\n",
    "    primes = [\n",
    "        1,\n",
    "        2654435761,\n",
    "        805459861,\n",
    "        3674653429,\n",
    "        2097192037,\n",
    "        1434869437,\n",
    "        2165219737,\n",
    "    ]\n",
    "\n",
    "    xor_result = torch.zeros_like(coords)[..., 0]\n",
    "    for i in range(coords.shape[-1]):\n",
    "        xor_result ^= coords[..., i] * primes[i]\n",
    "\n",
    "    return (\n",
    "        torch.tensor((1 << log2_hashmap_size) - 1, device=xor_result.device)\n",
    "        & xor_result\n",
    "        )\n",
    "\n",
    "def _to_1D(coords: torch.Tensor, resolution: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: 3D indices of grid\n",
    "    resolution:  resolution of grid\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Given grid resolution, for instance 2, our coordinate values usually span from 0 to 1 (inclusive, on x, y and z dimensions).\n",
    "    To convert this coordinate (which is between 0 and 1, inclusive) to a grid index,\n",
    "    we multiply the coordinate with the resolution (which is 2 in this example).\n",
    "    This means the maximum cell we can get is (2,2,2) when we multiply the coordinate (1,1,1) with resolution 2.\n",
    "    \n",
    "    If we want to convert the 3D cell index (2,2,2) into a 1D index (to retrieve the embedding),\n",
    "    we can use the formula (z * resolution * resolution) + (y * resolution) + x. The resolution here however must be 3,\n",
    "    since we are now dealing with a 3x3x3 grid. So, the 1D index is (2 * 3 * 3) + (2 * 3) + 2 = 26.\n",
    "    \n",
    "    If we use resolution 2, the 1D index would be (2 * 2 * 2) + (2 * 2) + 2 = 14. \n",
    "    This is however wrong, as it represents the wrong cell in a 3x3x3 grid.\n",
    "    \n",
    "    Now, we do resolution + 2 because we have offsets of + 1, so we can get a cell at location (3,3,3).\n",
    "    \n",
    "    \"\"\"\n",
    "    resolution = resolution + 2\n",
    "\n",
    "    x = coords[:, 0]\n",
    "    y = coords[:, 1]\n",
    "\n",
    "    return (y * resolution) + x\n",
    "\n",
    "\n",
    "def get_pixel_vertices(\n",
    "        xy: torch.Tensor, resolution: float\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        xy = xy * resolution\n",
    "        box_min_vertex = torch.floor(xy).int()\n",
    "        box_indices = box_min_vertex.unsqueeze(0) + box_offsets\n",
    "\n",
    "        max_size = 2**log2_hashmap_size\n",
    "        n_level_size = (resolution + 2) ** 3\n",
    "        if max_size > n_level_size: # No collisions, straightforward method\n",
    "            hashed_voxel_indices = _to_1D(box_indices, resolution)\n",
    "            \n",
    "        else: # Collisions are present\n",
    "            hashed_voxel_indices = _hash(box_indices, log2_hashmap_size)\n",
    "\n",
    "        return box_indices, hashed_voxel_indices, xy\n",
    "\n",
    "\n",
    "def bilinear_interp(\n",
    "    x: torch.Tensor,\n",
    "    box_indices: torch.Tensor,\n",
    "    box_embedds: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: B x 2\n",
    "    voxel_min_vertex: B x 2\n",
    "    voxel_max_vertex: B x 2\n",
    "    voxel_embedds: B x 4 x 2\n",
    "    \"\"\"\n",
    "    # source: https://en.wikipedia.org/wiki/Trilinear_interpolation\n",
    "    \n",
    "    w11 = np.linalg.norm(box_indices[0] - x)\n",
    "    w12 = np.linalg.norm(box_indices[1] - x)\n",
    "    w21 = np.linalg.norm(box_indices[2] - x)\n",
    "    w22 = np.linalg.norm(box_indices[3] - x)\n",
    "\n",
    "    den = w11+w12+w21+w22\n",
    "\n",
    "    w11 /= den\n",
    "    w12 /= den\n",
    "    w21 /= den\n",
    "    w22 /= den\n",
    "\n",
    "    xi_embedding = w11*box_embedds[0] + w12*box_embedds[1] + w21*box_embedds[2] + w22*box_embedds[3]  \n",
    "    return xi_embedding\n",
    "\n",
    "def _get_bbox (x, dx):\n",
    "    box_idx = torch.zeros((4,2))\n",
    "\n",
    "    box_idx[0,0] = (torch.floor(x[0]/dx)*dx)\n",
    "    box_idx[0,1] = (torch.floor(x[1]/dx)*dx)\n",
    "\n",
    "    box_idx[1,:] = box_idx[0,:]\n",
    "    box_idx[1,0] += dx\n",
    "\n",
    "    box_idx[3,0] = box_idx[0,0] + dx\n",
    "    box_idx[3,1] = box_idx[0,1] + dx\n",
    "\n",
    "    box_idx[2,:] = box_idx[3,:]\n",
    "    box_idx[2,0] -= dx\n",
    "        \n",
    "    return box_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Embedding(256, 2)\n",
      "  (1): Embedding(1024, 2)\n",
      "  (2): Embedding(4096, 2)\n",
      "  (3): Embedding(25600, 2)\n",
      "  (4): Embedding(102400, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embeddings = nn.ModuleList(\n",
    "    [\n",
    "        nn.Embedding(\n",
    "            _get_number_of_embeddings([i]), n_features_per_level\n",
    "        )\n",
    "        for i in range(5)\n",
    "    ]\n",
    ")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"bitwise_xor_cpu\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m n_level_size \u001b[38;5;241m=\u001b[39m (Nl[L]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# if max_size > n_level_size:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     hashed_box_idx = _to_1D(box_idx, Nl[i])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m hashed_box_idx \u001b[38;5;241m=\u001b[39m _hash(box_idx, log2_hashmap_size)\n\u001b[1;32m     20\u001b[0m voxel_embedds \u001b[38;5;241m=\u001b[39m embeddings[i](hashed_box_idx\u001b[38;5;241m.\u001b[39mint())\n",
      "Cell \u001b[0;32mIn[94], line 55\u001b[0m, in \u001b[0;36m_hash\u001b[0;34m(coords, log2_hashmap_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m xor_result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(coords)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(coords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 55\u001b[0m     xor_result \u001b[38;5;241m^\u001b[39m\u001b[38;5;241m=\u001b[39m coords[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i] \u001b[38;5;241m*\u001b[39m primes[i]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m log2_hashmap_size) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mxor_result\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;241m&\u001b[39m xor_result\n\u001b[1;32m     60\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"bitwise_xor_cpu\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "## DERIVE THE BOX SURROUNDING THE POINT AT ANY LEVEL\n",
    "L = 0\n",
    "Nmax = 320\n",
    "Nl = [16, 32, 64, 160, 320]\n",
    "for i in range(5):\n",
    "    dx = Nmax//Nl[i]\n",
    "\n",
    "    x = torch.tensor((1,2))\n",
    "\n",
    "    box_idx = _get_bbox(x, dx)\n",
    "\n",
    "    max_size = 2**10\n",
    "    n_level_size = (Nl[L]) ** 2\n",
    "\n",
    "    # if max_size > n_level_size:\n",
    "    #     hashed_box_idx = _to_1D(box_idx, Nl[i])\n",
    "    # else:\n",
    "    hashed_box_idx = _hash(box_idx, log2_hashmap_size)\n",
    "    \n",
    "    voxel_embedds = embeddings[i](hashed_box_idx.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_levels):\n\u001b[1;32m     14\u001b[0m     resolution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(base_resolution \u001b[38;5;241m*\u001b[39m b\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi)\n\u001b[0;32m---> 15\u001b[0m     (voxel_min_vertex, hashed_voxel_indices,xi) \u001b[38;5;241m=\u001b[39m get_voxel_vertices(x, resolution)\n",
      "Cell \u001b[0;32mIn[38], line 95\u001b[0m, in \u001b[0;36mget_voxel_vertices\u001b[0;34m(xyz, resolution)\u001b[0m\n\u001b[1;32m     92\u001b[0m xyz \u001b[38;5;241m=\u001b[39m xyz \u001b[38;5;241m*\u001b[39m resolution\n\u001b[1;32m     93\u001b[0m voxel_min_vertex \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloor(xyz)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m---> 95\u001b[0m voxel_indices \u001b[38;5;241m=\u001b[39m voxel_min_vertex\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m box_offsets\n\u001b[1;32m     97\u001b[0m max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog2_hashmap_size\n\u001b[1;32m     98\u001b[0m n_level_size \u001b[38;5;241m=\u001b[39m (resolution \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "x_embedded_all = []\n",
    "x = torch.tensor([120,180])\n",
    "\n",
    "embeddings = nn.ModuleList(\n",
    "    [\n",
    "        nn.Embedding(\n",
    "            _get_number_of_embeddings(i), n_features_per_level\n",
    "        )\n",
    "        for i in range(n_levels)\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(n_levels):\n",
    "    resolution = int(base_resolution * b**i)\n",
    "    (voxel_min_vertex, hashed_voxel_indices,xi) = get_voxel_vertices(x, resolution)\n",
    "    # voxel_embedds = embeddings[i](hashed_voxel_indices)\n",
    "    # x_embedded = trilinear_interp(xi, voxel_min_vertex, voxel_embedds)\n",
    "    # x_embedded_all.append(x_embedded)\n",
    "# return torch.cat(x_embedded_all, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hash_encoding_batch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 5\n",
    "n_min = 45\n",
    "n_features_per_level = 2\n",
    "n_max = 320\n",
    "log2_hashmap_size = 13\n",
    "b = np.exp((np.log(n_max) - np.log(n_min)) / (levels - 1))\n",
    "\n",
    "\n",
    "size  = 320\n",
    "x = torch.arange(size)\n",
    "y = torch.arange(size)\n",
    "z = torch.arange(4)\n",
    "coil = torch.arange(3)\n",
    "\n",
    "points = torch.meshgrid(x, y, z, coil, indexing=\"ij\")\n",
    "points = torch.stack(points, dim=-1).reshape(-1, len(points)).float()\n",
    "xy = points[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Embedding(2500, 2)\n",
      "  (1): Embedding(6084, 2)\n",
      "  (2-4): 3 x Embedding(8192, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def _get_number_of_embeddings(level_idx: int) -> int:\n",
    "    max_size = 2 ** log2_hashmap_size\n",
    "    n_l = int(n_min * (b ** level_idx).item())\n",
    "    n_l_embeddings = (n_l + 5) ** 2\n",
    "    return min(max_size, n_l_embeddings)\n",
    "\n",
    "def bilinear_interp(x: torch.Tensor, box_indices: torch.Tensor, box_embedds: torch.Tensor) -> torch.Tensor:\n",
    "    device = x.device\n",
    "    \n",
    "    if box_indices.shape[1] > 2:\n",
    "        weights = torch.norm(box_indices - x[:, None, :], dim=2)\n",
    "        den = weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        weights /= den # Normalize weights\n",
    "        weights = 1-weights # NOTE: More weight is given to vertex closer to the point of interest\n",
    "        \n",
    "        weights = weights.to(device)\n",
    "        box_embedds = box_embedds.to(device)\n",
    "\n",
    "        Npoints = len(den)\n",
    "        xi_embedding = torch.zeros((Npoints, 2), device = device)\n",
    "        \n",
    "        for i in range(4):\n",
    "            xi_embedding += weights[...,i].unsqueeze(1) * box_embedds[...,i,:]\n",
    "            \n",
    "    else:\n",
    "        xi_embedding = box_embedds\n",
    "        \n",
    "    return xi_embedding\n",
    "\n",
    "def _get_box_idx(points: torch.Tensor, n_l: int) -> tuple:\n",
    "    \n",
    "    # Get bounding box indices for a batch of points\n",
    "    if points.dim() > 1:\n",
    "        x = points[:,0]\n",
    "        y = points[:,1]\n",
    "    else:\n",
    "        x = points[0]\n",
    "        y = points[1]\n",
    "\n",
    "    if n_max == n_l:\n",
    "        box_idx = points\n",
    "        hashed_box_idx = _hash(points)\n",
    "    else:\n",
    "        # Calculate box size based on the total boxes\n",
    "        box_width = n_max // n_l  # Width of each box\n",
    "        box_height = n_max // n_l  # Height of each box\n",
    "\n",
    "        x_min = torch.maximum(torch.zeros_like(x), (x // box_width) * box_width)\n",
    "        y_min = torch.maximum(torch.zeros_like(y), (y // box_height) * box_height)\n",
    "        x_max = torch.minimum(torch.full_like(x, n_max), x_min + box_width)\n",
    "        y_max = torch.minimum(torch.full_like(y, n_max), y_min + box_height)\n",
    "        \n",
    "        # Stack to create four corners per point, maintaining the batch dimension\n",
    "        box_idx = torch.stack([\n",
    "            torch.stack([x_min, y_min], dim=1),\n",
    "            torch.stack([x_max, y_min], dim=1),\n",
    "            torch.stack([x_min, y_max], dim=1),\n",
    "            torch.stack([x_max, y_max], dim=1)\n",
    "        ], dim=1)  # Shape: (batch_size, 4, 2)\n",
    "        \n",
    "        # Determine if the coordinates can be directly mapped or need hashing\n",
    "        max_hashtable_size = 2 ** log2_hashmap_size\n",
    "        if max_hashtable_size >= (n_l + 5) ** 2:\n",
    "            hashed_box_idx, _ = _to_1D(box_idx, n_l)\n",
    "        else:\n",
    "            hashed_box_idx = _hash(box_idx)\n",
    "            \n",
    "    return box_idx, hashed_box_idx\n",
    "\n",
    "## Hash encoders\n",
    "def _to_1D(coors, n_l):\n",
    "\n",
    "    scale_factor = n_max // n_l\n",
    "    scaled_coords = torch.div(coors, scale_factor, rounding_mode=\"floor\").int()    \n",
    "    x = scaled_coords[...,0]\n",
    "    y = scaled_coords[...,1]\n",
    "    \n",
    "    return (y * n_l + x), scaled_coords\n",
    "\n",
    "\n",
    "def _hash(coords: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: this function can process upto 7 dim coordinates\n",
    "    log2T:  logarithm of T w.r.t 2\n",
    "    \"\"\"\n",
    "    device = coords.device\n",
    "    primes = torch.tensor([\n",
    "        1,\n",
    "        2654435761,\n",
    "        805459861,\n",
    "        3674653429,\n",
    "        2097192037,\n",
    "        1434869437,\n",
    "        2165219737,\n",
    "    ], dtype = torch.int64, device=device\n",
    "    )\n",
    "\n",
    "    xor_result = torch.zeros(coords.shape[:-1], dtype=torch.int64, device=device)\n",
    "\n",
    "    for i in range(coords.shape[-1]): # Loop around all possible dimensions of the vector containing the bounding box positions\n",
    "        xor_result ^= coords[...,i].to(torch.int64)*primes[i]\n",
    "        \n",
    "    hash_mask = (1 << log2_hashmap_size) - 1\n",
    "    return xor_result & hash_mask\n",
    "\n",
    "\n",
    "embeddings = nn.ModuleList([\n",
    "            nn.Embedding(_get_number_of_embeddings(i), n_features_per_level)\n",
    "            for i in range(levels)])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No collision existed : 16, 441\n",
      "Reality : 16, 441\n",
      "No collision existed : 33, 1444\n",
      "Reality : 33, 1444\n",
      "No collision existed : 71, 5776\n",
      "Reality : 71, 5776\n",
      "No collision existed : 151, 24336\n",
      "Reality : 151, 8192\n",
      "No collision existed : 319, 104976\n",
      "Reality : 319, 8192\n"
     ]
    }
   ],
   "source": [
    "n_min = 16\n",
    "levels = 5\n",
    "\n",
    "b = np.exp((np.log(n_max) - np.log(n_min)) / (levels - 1))\n",
    "\n",
    "for i in range(levels):\n",
    "    n_l = int(n_min * b ** i)\n",
    "    print(f'No collision existed : {n_l}, {(n_l+5)**2}')\n",
    "    print(f'Reality : {n_l}, {_get_number_of_embeddings(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_embedded_all = []\n",
    "\n",
    "for i in range(levels):\n",
    "    n_l = int(n_min * b ** i)\n",
    "    \n",
    "    box_idx, hashed_box_idx = _get_box_idx(xy, n_l)\n",
    "    \n",
    "    box_embedds = embeddings[i](hashed_box_idx)\n",
    "    \n",
    "    xy_embedded = bilinear_interp(xy, box_idx, box_embedds)\n",
    "    \n",
    "    xy_embedded_all.append(xy_embedded)\n",
    "    \n",
    "    \n",
    "xy_embedded_all = torch.cat(xy_embedded_all, dim = 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,   56,   57],\n",
       "        [   0,    1,   56,   57],\n",
       "        [   0,    1,   56,   57],\n",
       "        ...,\n",
       "        [3591, 3592, 3647, 3648],\n",
       "        [3591, 3592, 3647, 3648],\n",
       "        [3591, 3592, 3647, 3648]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_box_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

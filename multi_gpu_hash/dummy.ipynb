{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fastmri\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from data_utils import *\n",
    "from datasets import *\n",
    "from fastmri.data.transforms import tensor_to_complex_np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model import *\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_to_data = '/itet-stor/mcrespo/bmicdatasets-originals/Originals/fastMRI/brain/multicoil_train/'\n",
    "n_volumes = 1\n",
    "\n",
    "dataset = KCoordDataset(path_to_data=path_to_data, n_volumes=4, n_slices=2 ,with_mask=False, center_frac=0.15)\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=120_000,\n",
    "        num_workers=0, # This is needed to make processing faster \n",
    "        shuffle=False,\n",
    "        # sampler=DistributedSampler(dataset),\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(76, 64)\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# volume embedding\n",
    "#####################################################################\n",
    "embeddings_vol = torch.nn.Embedding(\n",
    "    len(dataset.metadata), 256\n",
    ")\n",
    "torch.nn.init.normal_(\n",
    "    embeddings_vol.weight.data, 0.0, 0.001\n",
    ")\n",
    "\n",
    "coil_size = []\n",
    "for i in range(len(dataset.metadata)):\n",
    "    _, n_coils, _, _ = dataset.metadata[i][\"shape\"]\n",
    "    coil_size.append(n_coils)\n",
    "########################################################\n",
    "total_n_coils = torch.cumsum(torch.tensor(coil_size), dim=0)[-1]\n",
    "\n",
    "start_indx = torch.tensor([0] + list(torch.cumsum(torch.tensor(coil_size), dim=0)[:-1]))\n",
    "\n",
    "embeddings_coil = torch.nn.Embedding(total_n_coils.item(), 64)\n",
    "torch.nn.init.normal_(\n",
    "    embeddings_coil.weight.data, 0.0, 0.01\n",
    ")\n",
    "\n",
    "print(embeddings_coil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "    # inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "    \n",
    "    coords = inputs[:, 1:]\n",
    "    vol_ids = inputs[:, 0].long()\n",
    "    coil_id = inputs[:,-1].long()\n",
    "    \n",
    "    ## Map the coil_id to the embedding_coil_id table\n",
    "    idx_start_coil = start_indx[vol_ids]\n",
    "    coil_index_table = idx_start_coil + coil_id\n",
    "    \n",
    "    if batch_idx == 0:\n",
    "        break\n",
    "    \n",
    "    # col_ids = inputs[:, -1].long()\n",
    "    # shape = dataloader.dataset.metadata[vol_ids][\"shape\"]\n",
    "    # latent_vol = embeddings_vol(vol_ids)\n",
    "    # latent_coil = embeddings_coil[vol_ids](col_ids.unsqueeze(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_coil = embeddings_coil(coil_index_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.1697e-04,  1.6054e-02, -8.1535e-03, -4.3804e-03, -1.4798e-03,\n",
      "         1.0080e-02,  1.5267e-03, -7.8560e-03, -1.4763e-02, -6.3897e-03,\n",
      "         2.2738e-03, -4.8656e-03, -1.0216e-02,  2.7503e-02,  1.4525e-02,\n",
      "         1.9802e-03, -5.4871e-03,  6.7677e-03,  1.6262e-02,  2.9267e-03,\n",
      "         1.8971e-02, -1.1292e-02,  5.8535e-03, -8.5662e-03,  4.1489e-03,\n",
      "        -6.2677e-03,  9.8695e-03,  9.1102e-03,  1.2742e-02, -2.8866e-02,\n",
      "         1.7633e-03, -1.4269e-02, -2.2633e-03, -1.1309e-02, -2.3321e-03,\n",
      "        -3.9699e-03,  1.6685e-02, -1.3623e-02,  1.0326e-03,  1.3341e-02,\n",
      "        -1.6296e-02,  1.0357e-02,  2.2920e-03,  8.5684e-03, -1.2156e-02,\n",
      "        -4.3018e-03,  2.7980e-02,  5.8493e-03,  1.1699e-02, -1.1763e-02,\n",
      "         1.5692e-03, -1.0518e-02,  1.2219e-02,  2.8622e-03,  3.3609e-04,\n",
      "        -1.2874e-02,  2.4047e-02,  4.8773e-05,  1.3925e-03, -3.4696e-03,\n",
      "        -6.3902e-03,  5.2315e-03,  2.9739e-03,  1.3944e-02],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0072, -0.0039,  0.0004,  0.0150, -0.0051,  0.0007,  0.0084,  0.0052,\n",
      "         0.0052,  0.0021, -0.0093, -0.0164,  0.0007, -0.0093, -0.0078, -0.0218,\n",
      "         0.0072,  0.0077,  0.0029,  0.0107,  0.0034,  0.0015,  0.0031,  0.0135,\n",
      "        -0.0034, -0.0086, -0.0202,  0.0079, -0.0083, -0.0020, -0.0081,  0.0028,\n",
      "         0.0170, -0.0151,  0.0065,  0.0060,  0.0205, -0.0135, -0.0036, -0.0174,\n",
      "         0.0039,  0.0205, -0.0133,  0.0043, -0.0028, -0.0224, -0.0019, -0.0046,\n",
      "        -0.0029,  0.0166,  0.0054, -0.0190,  0.0067, -0.0089,  0.0024, -0.0044,\n",
      "         0.0008,  0.0041, -0.0024,  0.0015,  0.0010, -0.0138, -0.0140, -0.0070],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(latent_coil[0])\n",
    "print(latent_coil[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmetadata[np\u001b[38;5;241m.\u001b[39mndarray(vol_ids[\u001b[38;5;241m0\u001b[39m])]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.metadata["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_coors):\n\u001b[0;32m---> 24\u001b[0m     vol_ids \u001b[38;5;241m=\u001b[39m coors[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# Batch of volume IDs\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     coil_ids \u001b[38;5;241m=\u001b[39m coors[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# Batch of coil IDs\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     embedding_vector \u001b[38;5;241m=\u001b[39m embeddings[vol_ids](coil_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Lookup for vol_id and coil_id\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of coils (Nc) for each volume and the embedding size\n",
    "num_coils_per_volume = [20, 20, 16]  # Replace with actual Nc values\n",
    "embedding_dim = 64\n",
    "\n",
    "# Create a list of nn.Embedding for each volume\n",
    "embeddings = nn.ModuleList([\n",
    "    nn.Embedding(num_coils, embedding_dim) for num_coils in num_coils_per_volume\n",
    "])\n",
    "\n",
    "# Example input from dataloader\n",
    "# input_coors = [vol_ID, kx, ky, kz, coilID] (batch_size, 5)\n",
    "input_coors = torch.tensor([[0, 0.1, 0.2, 0.3, 5],  # vol_ID = 0, coilID = 5\n",
    "                            [1, 0.5, 0.6, 0.7, 12],  # vol_ID = 1, coilID = 12\n",
    "                            [2, 0.8, 0.9, 1.0, 7]])  # vol_ID = 2, coilID = 7\n",
    "\n",
    "# Extract vol_IDs and coil_IDs from input_coors\n",
    "\n",
    "# Fetch embeddings for each input coordinate\n",
    "batch_embeddings = []\n",
    "for coors in enumerate(input_coors):\n",
    "    vol_ids = coors[:, 0].long()  # Batch of volume IDs\n",
    "    coil_ids = coors[:, -1].long()  # Batch of coil IDs\n",
    "\n",
    "    embedding_vector = embeddings[vol_ids](coil_ids.unsqueeze(0))  # Lookup for vol_id and coil_id\n",
    "    batch_embeddings.append(embedding_vector)\n",
    "\n",
    "# Combine embeddings into a batch tensor\n",
    "batch_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "print(batch_embeddings.shape)  # (batch_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:,0].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

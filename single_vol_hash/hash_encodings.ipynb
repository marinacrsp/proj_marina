{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hash_encoding_batch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 5\n",
    "n_min = 45\n",
    "n_features_per_level = 2\n",
    "n_max = 320\n",
    "log2_hashmap_size = 13\n",
    "b = np.exp((np.log(n_max) - np.log(n_min)) / (levels - 1))\n",
    "\n",
    "\n",
    "size  = 320\n",
    "x = torch.arange(size)\n",
    "y = torch.arange(size)\n",
    "z = torch.arange(4)\n",
    "coil = torch.arange(3)\n",
    "\n",
    "points = torch.meshgrid(x, y, z, coil, indexing=\"ij\")\n",
    "points = torch.stack(points, dim=-1).reshape(-1, len(points)).float()\n",
    "xy = points[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training w/o center\n",
      "training w/o center\n",
      "training w/o center\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import *\n",
    "folder_data = '/itet-stor/mcrespo/bmicdatasets-originals/Originals/fastMRI/brain/multicoil_train/'\n",
    "file = 'file_brain_AXT1POST_203_6000861.h5'\n",
    "file_data = os.path.join(folder_data, file)\n",
    "\n",
    "dataset = KCoordDataset(path_to_data=folder_data, with_mask=False, acceleration=3,n_volumes=3, center_train=False, mask_type='Equispaced' )\n",
    "\n",
    "embeddings = torch.nn.Embedding(\n",
    "    len(dataset.metadata), 512\n",
    ")\n",
    "\n",
    "coord_dim  =4\n",
    "L = 10\n",
    "\n",
    "L_mult = torch.pow(2, torch.arange(L)) * np.pi\n",
    "# register_buffer(\"L_mult\", L_mult)\n",
    "coord_encoding_dim = L * 2 * coord_dim\n",
    "\n",
    "x = points.unsqueeze(-1) * L_mult\n",
    "x = torch.cat([torch.sin(x), torch.cos(x)], dim=-1)\n",
    "x = x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.],\n",
       "        [  0.,   0.,   1.],\n",
       "        [  0.,   0.,   2.],\n",
       "        ...,\n",
       "        [319.,   3.,   0.],\n",
       "        [319.,   3.,   1.],\n",
       "        [319.,   3.,   2.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point,_ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 12\u001b[0m     coords, latent_embeddings \u001b[38;5;241m=\u001b[39m point[:, \u001b[38;5;241m1\u001b[39m:], embeddings(point[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     14\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch_net/ken/mcrespo/conda_envs/pytcu11/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch_net/ken/mcrespo/conda_envs/pytcu11/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch_net/ken/mcrespo/conda_envs/pytcu11/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m/scratch_net/ken/mcrespo/conda_envs/pytcu11/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=60_000,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    ")\n",
    "counter = 0\n",
    "for point,_ in dataloader:\n",
    "    coords, latent_embeddings = point[:, 1:], embeddings(point[:, 0].long())\n",
    "    \n",
    "    counter += 1\n",
    "    if counter > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Embedding(2500, 2)\n",
      "  (1): Embedding(6084, 2)\n",
      "  (2-4): 3 x Embedding(8192, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def _get_number_of_embeddings(level_idx: int) -> int:\n",
    "    max_size = 2 ** log2_hashmap_size\n",
    "    n_l = int(n_min * (b ** level_idx).item())\n",
    "    n_l_embeddings = (n_l + 5) ** 2\n",
    "    return min(max_size, n_l_embeddings)\n",
    "\n",
    "def bilinear_interp(x: torch.Tensor, box_indices: torch.Tensor, box_embedds: torch.Tensor) -> torch.Tensor:\n",
    "    device = x.device\n",
    "    \n",
    "    if box_indices.shape[1] > 2:\n",
    "        weights = torch.norm(box_indices - x[:, None, :], dim=2)\n",
    "        den = weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        weights /= den # Normalize weights\n",
    "        weights = 1-weights # NOTE: More weight is given to vertex closer to the point of interest\n",
    "        \n",
    "        weights = weights.to(device)\n",
    "        box_embedds = box_embedds.to(device)\n",
    "\n",
    "        Npoints = len(den)\n",
    "        xi_embedding = torch.zeros((Npoints, 2), device = device)\n",
    "        \n",
    "        for i in range(4):\n",
    "            xi_embedding += weights[...,i].unsqueeze(1) * box_embedds[...,i,:]\n",
    "            \n",
    "    else:\n",
    "        xi_embedding = box_embedds\n",
    "        \n",
    "    return xi_embedding\n",
    "\n",
    "def _get_box_idx(points: torch.Tensor, n_l: int) -> tuple:\n",
    "    \n",
    "    # Get bounding box indices for a batch of points\n",
    "    if points.dim() > 1:\n",
    "        x = points[:,0]\n",
    "        y = points[:,1]\n",
    "    else:\n",
    "        x = points[0]\n",
    "        y = points[1]\n",
    "\n",
    "    if n_max == n_l:\n",
    "        box_idx = points\n",
    "        hashed_box_idx = _hash(points)\n",
    "    else:\n",
    "        # Calculate box size based on the total boxes\n",
    "        box_width = n_max // n_l  # Width of each box\n",
    "        box_height = n_max // n_l  # Height of each box\n",
    "\n",
    "        x_min = torch.maximum(torch.zeros_like(x), (x // box_width) * box_width)\n",
    "        y_min = torch.maximum(torch.zeros_like(y), (y // box_height) * box_height)\n",
    "        x_max = torch.minimum(torch.full_like(x, n_max), x_min + box_width)\n",
    "        y_max = torch.minimum(torch.full_like(y, n_max), y_min + box_height)\n",
    "        \n",
    "        # Stack to create four corners per point, maintaining the batch dimension\n",
    "        box_idx = torch.stack([\n",
    "            torch.stack([x_min, y_min], dim=1),\n",
    "            torch.stack([x_max, y_min], dim=1),\n",
    "            torch.stack([x_min, y_max], dim=1),\n",
    "            torch.stack([x_max, y_max], dim=1)\n",
    "        ], dim=1)  # Shape: (batch_size, 4, 2)\n",
    "        \n",
    "        # Determine if the coordinates can be directly mapped or need hashing\n",
    "        max_hashtable_size = 2 ** log2_hashmap_size\n",
    "        if max_hashtable_size >= (n_l + 5) ** 2:\n",
    "            hashed_box_idx, _ = _to_1D(box_idx, n_l)\n",
    "        else:\n",
    "            hashed_box_idx = _hash(box_idx)\n",
    "            \n",
    "    return box_idx, hashed_box_idx\n",
    "\n",
    "## Hash encoders\n",
    "def _to_1D(coors, n_l):\n",
    "\n",
    "    scale_factor = n_max // n_l\n",
    "    scaled_coords = torch.div(coors, scale_factor, rounding_mode=\"floor\").int()    \n",
    "    x = scaled_coords[...,0]\n",
    "    y = scaled_coords[...,1]\n",
    "    \n",
    "    return (y * n_l + x), scaled_coords\n",
    "\n",
    "\n",
    "def _hash(coords: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: this function can process upto 7 dim coordinates\n",
    "    log2T:  logarithm of T w.r.t 2\n",
    "    \"\"\"\n",
    "    device = coords.device\n",
    "    primes = torch.tensor([\n",
    "        1,\n",
    "        2654435761,\n",
    "        805459861,\n",
    "        3674653429,\n",
    "        2097192037,\n",
    "        1434869437,\n",
    "        2165219737,\n",
    "    ], dtype = torch.int64, device=device\n",
    "    )\n",
    "\n",
    "    xor_result = torch.zeros(coords.shape[:-1], dtype=torch.int64, device=device)\n",
    "\n",
    "    for i in range(coords.shape[-1]): # Loop around all possible dimensions of the vector containing the bounding box positions\n",
    "        xor_result ^= coords[...,i].to(torch.int64)*primes[i]\n",
    "        \n",
    "    hash_mask = (1 << log2_hashmap_size) - 1\n",
    "    return xor_result & hash_mask\n",
    "\n",
    "\n",
    "embeddings = nn.ModuleList([\n",
    "            nn.Embedding(_get_number_of_embeddings(i), n_features_per_level)\n",
    "            for i in range(levels)])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No collision existed : 16, 441\n",
      "Reality : 16, 441\n",
      "No collision existed : 33, 1444\n",
      "Reality : 33, 1444\n",
      "No collision existed : 71, 5776\n",
      "Reality : 71, 5776\n",
      "No collision existed : 151, 24336\n",
      "Reality : 151, 8192\n",
      "No collision existed : 319, 104976\n",
      "Reality : 319, 8192\n"
     ]
    }
   ],
   "source": [
    "n_min = 16\n",
    "levels = 5\n",
    "\n",
    "b = np.exp((np.log(n_max) - np.log(n_min)) / (levels - 1))\n",
    "\n",
    "for i in range(levels):\n",
    "    n_l = int(n_min * b ** i)\n",
    "    print(f'No collision existed : {n_l}, {(n_l+5)**2}')\n",
    "    print(f'Reality : {n_l}, {_get_number_of_embeddings(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_embedded_all = []\n",
    "\n",
    "for i in range(levels):\n",
    "    n_l = int(n_min * b ** i)\n",
    "    \n",
    "    box_idx, hashed_box_idx = _get_box_idx(xy, n_l)\n",
    "    \n",
    "    box_embedds = embeddings[i](hashed_box_idx)\n",
    "    \n",
    "    xy_embedded = bilinear_interp(xy, box_idx, box_embedds)\n",
    "    \n",
    "    xy_embedded_all.append(xy_embedded)\n",
    "    \n",
    "    \n",
    "xy_embedded_all = torch.cat(xy_embedded_all, dim = 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,   56,   57],\n",
       "        [   0,    1,   56,   57],\n",
       "        [   0,    1,   56,   57],\n",
       "        ...,\n",
       "        [3591, 3592, 3647, 3648],\n",
       "        [3591, 3592, 3647, 3648],\n",
       "        [3591, 3592, 3647, 3648]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_box_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytcu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
